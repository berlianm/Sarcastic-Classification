{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "vUQFUV7ktL0B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m0k74eEFtI4W"
      },
      "outputs": [],
      "source": [
        "def sarcasm_classification():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sarcasm.json'\n",
        "    urllib.request.urlretrieve(data_url, 'sarcasm.json')\n",
        "\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    padding_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    with open(\"sarcasm.json\", 'r') as f:\n",
        "        datastore = json.load(f)\n",
        "\n",
        "    for item in datastore:\n",
        "        sentences.append(item['headline'])\n",
        "        labels.append(item['is_sarcastic'])\n",
        "\n",
        "    training_sentences = sentences[0:training_size]\n",
        "    testing_sentences = sentences[training_size:]\n",
        "\n",
        "    training_labels = labels[0:training_size]\n",
        "    testing_labels = labels[training_size:]\n",
        "\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size,\n",
        "                          oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences,\n",
        "                                    maxlen=max_length,\n",
        "                                    padding=padding_type,\n",
        "                                    truncating=trunc_type)\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(testing_sequences,\n",
        "                                   maxlen=max_length,\n",
        "                                   padding=padding_type,\n",
        "                                   truncating=trunc_type)\n",
        "\n",
        "    training_label_seq = np.array(training_labels)\n",
        "    testing_label_seq = np.array(testing_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size,\n",
        "                                  embedding_dim,\n",
        "                                  input_length=max_length),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(16)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(training_padded,\n",
        "                        training_label_seq,\n",
        "                        epochs=25,\n",
        "                        validation_data=(testing_padded, testing_label_seq), verbose=2)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    model = sarcasm_classification()\n",
        "    model.save(\"model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G_DEhcQtxGU",
        "outputId": "4f18957f-3297-40d8-94ec-bb4491f16ca9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "625/625 - 82s - loss: 0.4566 - accuracy: 0.7724 - val_loss: 0.3822 - val_accuracy: 0.8234 - 82s/epoch - 132ms/step\n",
            "Epoch 2/25\n",
            "625/625 - 78s - loss: 0.3518 - accuracy: 0.8413 - val_loss: 0.3769 - val_accuracy: 0.8283 - 78s/epoch - 125ms/step\n",
            "Epoch 3/25\n",
            "625/625 - 74s - loss: 0.3293 - accuracy: 0.8523 - val_loss: 0.3739 - val_accuracy: 0.8243 - 74s/epoch - 119ms/step\n",
            "Epoch 4/25\n",
            "625/625 - 79s - loss: 0.3164 - accuracy: 0.8594 - val_loss: 0.3816 - val_accuracy: 0.8283 - 79s/epoch - 126ms/step\n",
            "Epoch 5/25\n",
            "625/625 - 78s - loss: 0.3085 - accuracy: 0.8619 - val_loss: 0.3849 - val_accuracy: 0.8202 - 78s/epoch - 124ms/step\n",
            "Epoch 6/25\n",
            "625/625 - 77s - loss: 0.3001 - accuracy: 0.8669 - val_loss: 0.3765 - val_accuracy: 0.8305 - 77s/epoch - 123ms/step\n",
            "Epoch 7/25\n",
            "625/625 - 75s - loss: 0.2861 - accuracy: 0.8752 - val_loss: 0.3855 - val_accuracy: 0.8331 - 75s/epoch - 119ms/step\n",
            "Epoch 8/25\n",
            "625/625 - 78s - loss: 0.2774 - accuracy: 0.8795 - val_loss: 0.3890 - val_accuracy: 0.8247 - 78s/epoch - 125ms/step\n",
            "Epoch 9/25\n",
            "625/625 - 79s - loss: 0.2659 - accuracy: 0.8836 - val_loss: 0.4015 - val_accuracy: 0.8246 - 79s/epoch - 126ms/step\n",
            "Epoch 10/25\n",
            "625/625 - 74s - loss: 0.2568 - accuracy: 0.8882 - val_loss: 0.3963 - val_accuracy: 0.8229 - 74s/epoch - 118ms/step\n",
            "Epoch 11/25\n",
            "625/625 - 74s - loss: 0.2505 - accuracy: 0.8917 - val_loss: 0.4076 - val_accuracy: 0.8269 - 74s/epoch - 118ms/step\n",
            "Epoch 12/25\n",
            "625/625 - 79s - loss: 0.2417 - accuracy: 0.8954 - val_loss: 0.4407 - val_accuracy: 0.8232 - 79s/epoch - 126ms/step\n",
            "Epoch 13/25\n",
            "625/625 - 74s - loss: 0.2368 - accuracy: 0.8982 - val_loss: 0.4321 - val_accuracy: 0.8198 - 74s/epoch - 118ms/step\n",
            "Epoch 14/25\n",
            "625/625 - 74s - loss: 0.2302 - accuracy: 0.9025 - val_loss: 0.4650 - val_accuracy: 0.8185 - 74s/epoch - 118ms/step\n",
            "Epoch 15/25\n",
            "625/625 - 71s - loss: 0.2209 - accuracy: 0.9053 - val_loss: 0.4909 - val_accuracy: 0.8228 - 71s/epoch - 113ms/step\n",
            "Epoch 16/25\n",
            "625/625 - 74s - loss: 0.2155 - accuracy: 0.9075 - val_loss: 0.5016 - val_accuracy: 0.8255 - 74s/epoch - 119ms/step\n",
            "Epoch 17/25\n",
            "625/625 - 77s - loss: 0.2082 - accuracy: 0.9105 - val_loss: 0.5100 - val_accuracy: 0.8216 - 77s/epoch - 124ms/step\n",
            "Epoch 18/25\n",
            "625/625 - 73s - loss: 0.2015 - accuracy: 0.9153 - val_loss: 0.5669 - val_accuracy: 0.8199 - 73s/epoch - 117ms/step\n",
            "Epoch 19/25\n",
            "625/625 - 76s - loss: 0.1944 - accuracy: 0.9153 - val_loss: 0.5508 - val_accuracy: 0.8201 - 76s/epoch - 122ms/step\n",
            "Epoch 20/25\n",
            "625/625 - 74s - loss: 0.1871 - accuracy: 0.9197 - val_loss: 0.6115 - val_accuracy: 0.8173 - 74s/epoch - 118ms/step\n",
            "Epoch 21/25\n",
            "625/625 - 74s - loss: 0.1815 - accuracy: 0.9229 - val_loss: 0.6196 - val_accuracy: 0.8173 - 74s/epoch - 118ms/step\n",
            "Epoch 22/25\n",
            "625/625 - 74s - loss: 0.1739 - accuracy: 0.9234 - val_loss: 0.6228 - val_accuracy: 0.8149 - 74s/epoch - 118ms/step\n",
            "Epoch 23/25\n",
            "625/625 - 72s - loss: 0.1662 - accuracy: 0.9284 - val_loss: 0.6771 - val_accuracy: 0.8137 - 72s/epoch - 116ms/step\n",
            "Epoch 24/25\n",
            "625/625 - 74s - loss: 0.1629 - accuracy: 0.9288 - val_loss: 0.6935 - val_accuracy: 0.8114 - 74s/epoch - 118ms/step\n",
            "Epoch 25/25\n",
            "625/625 - 77s - loss: 0.1588 - accuracy: 0.9297 - val_loss: 0.7072 - val_accuracy: 0.8120 - 77s/epoch - 123ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}